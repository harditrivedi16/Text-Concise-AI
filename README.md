# **Text Summarization with Fine-Tuned Pegasus Model**

## **Overview**
In this project, I fine-tuned the Pegasus model for a specialized text summarization task. The task aimed to generate concise summaries of long-form text data using a custom dataset that I curated for this purpose. Utilizing the Hugging Face API, I built an end-to-end pipeline that includes data ingestion, validation, transformation, model training, and evaluation. The entire solution was deployed on an AWS EC2 instance and leveraged a continuous integration/continuous deployment (CI/CD) pipeline to ensure smooth model updates and deployment.

The model achieved a ROUGE score of 0.4, demonstrating its capability in generating meaningful summaries. Additionally, a user-facing application was built using FastAPI, enabling users to interact with the summarization model via a simple web interface. This project highlights the integration of machine learning and cloud infrastructure, showcasing my proficiency in deploying and scaling ML models in a production environment.

## **Tech Stack**
- **Machine Learning Frameworks:** 
  - **Hugging Face Transformers**: Used for fine-tuning the Pegasus model for text summarization.
  - **PyTorch**: Underlying deep learning framework for model training.
- **Web Development:**
  - **FastAPI**: A modern web framework for building the user-facing application to interact with the summarization model.
- **Cloud Infrastructure:**
  - **AWS EC2**: Deployed the final model on AWS EC2 for scalable cloud-based hosting.
- **CI/CD:**
  - **GitHub Actions**: Used to automate model training, testing, and deployment through CI/CD pipelines.
- **Data Processing & Validation:**
  - **Pandas & NumPy**: Used for data manipulation and validation during the pre-processing stage.
- **Model Evaluation:**
  - **ROUGE Score**: Metrics used to evaluate the performance of the model in terms of summary quality.

## **Key Features**
1. **Custom Dataset Integration:** 
   - Curated a specialized dataset for training the model, ensuring the summarization task was aligned with real-world text data.
2. **End-to-End Machine Learning Pipeline:** 
   - The pipeline efficiently handles data ingestion, preprocessing, model training, and evaluation, ensuring a seamless flow from raw data to final deployment.
3. **Fine-Tuned Pegasus Model:**
   - Utilized the Pegasus model from Hugging Face, a state-of-the-art transformer model optimized for abstractive summarization.
4. **Evaluation with ROUGE Metrics:** 
   - The model’s performance was evaluated using ROUGE scores, which are standard metrics for assessing the quality of automatic summaries.
5. **Cloud Deployment on AWS EC2:**
   - Deployed the fine-tuned model on AWS EC2 for scalable and reliable hosting, ensuring high availability and quick response times.
6. **CI/CD Pipeline for Automated Deployment:** 
   - Integrated a CI/CD process using GitHub Actions, ensuring smooth updates and deployment of the model, reducing downtime.
7. **User-Facing Web Application:** 
   - A FastAPI application was built to provide users with an easy-to-use interface to input text and receive summaries generated by the model.

## **Usage**
1. **Clone the repository:**
   ```bash
   git clone https://github.com/your-username/text-summarization-pegasus.git
   ```
2. **Install the dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
3. **Run the FastAPI application locally:**
   ```bash
   uvicorn app:app --reload
   ```
   This will start the FastAPI server locally, and you can access the application at `http://127.0.0.1:8000`.
4. **Input text and get a summary:**
   - Navigate to the provided API endpoint (e.g., `/summarize`), and provide the text you’d like to summarize. The system will return an abridged summary using the fine-tuned Pegasus model.
5. **Deploy on AWS (Optional):**
   - If you'd like to deploy the solution on AWS EC2, follow the deployment instructions in the `docs/DEPLOYMENT.md` file to set up an EC2 instance, configure your environment, and deploy the model.

## **Model Evaluation**
The model’s performance is assessed using the **ROUGE score**, a standard metric in summarization tasks. It measures the overlap between the model-generated summary and the reference summary, considering different types of n-grams, synonyms, and sentence structures. The final ROUGE score achieved by this model is **0.4**, which is a promising result for a specialized summarization task.
